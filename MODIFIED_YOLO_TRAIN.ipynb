{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train.py\n",
    "from models import *\n",
    "from utils.utils import *\n",
    "from utils.datasets import *\n",
    "from utils.parse_config import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "from config import myolo_config\n",
    "from mutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastprogress import master_bar, progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=myolo_config(\"/media/tianning/DATA/COCO/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.batch_size=1\n",
    "# Get data configuration\n",
    "data_config = parse_data_config(opt.data_config_path)\n",
    "train_path = opt.train\n",
    "INIT_MODEL  =False\n",
    "INIT_KERNEL =False\n",
    "KERNEL_TRAIN=False\n",
    "# Get dataloader\n",
    "dataloader = torch.utils.data.DataLoader(ListDataset(train_path), batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n",
    "cuda = torch.cuda.is_available() and opt.use_cuda\n",
    "classes = load_classes(opt.class_path)\n",
    "\n",
    "model =MDarknet('config/m-yolov3.cfg')\n",
    "#model =Darknet('config/yolov3.cfg')\n",
    "#model =MDarknet('config/f-yolov3.cfg')\n",
    "#model.load_weights('../PyTorch-YOLOv3-RPN/weights/yolov3.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),'origin.yolo.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#state_dict=torch.load('origin.yolo.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INIT_MODEL:\n",
    "    model.apply(weights_init_normal)\n",
    "else:\n",
    "    model.load_weights(\"checkpoints/latest.weights\")\n",
    "    #model.load_state_dict(torch.load('origin.fyolo.weights'))\n",
    "if cuda:model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(),'origin.yolo.weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "if INIT_KERNEL:\n",
    "    cls_k_b_pool={}\n",
    "    feed_cls_k_b=[generate_new_kernels(cls_k_b_pool,classes,1,1024,cuda),\n",
    "              generate_new_kernels(cls_k_b_pool,classes,2,512,cuda),\n",
    "              generate_new_kernels(cls_k_b_pool,classes,3,256,cuda)]\n",
    "else:\n",
    "    cls_k_b_pool=torch.load('checkpoints/start.plug_in.weights')\n",
    "    feed_cls_k_b=review_plug_in(cls_k_b_pool,classes,cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernel_param=[x for l in feed_cls_k_b for kb in l for x in kb]\n",
    "if KERNEL_TRAIN:\n",
    "    for p in model.parameters():p.requires_grad=False\n",
    "    for p in all_kernel_param: p.requires_grad =True\n",
    "else:\n",
    "    for p in model.parameters():p.requires_grad=True\n",
    "    for p in all_kernel_param: p.requires_grad =True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get hyper parameters\n",
    "hyperparams   = parse_model_config(opt.model_config_path)[0]\n",
    "learning_rate = float(hyperparams[\"learning_rate\"])\n",
    "momentum      = float(hyperparams[\"momentum\"])\n",
    "decay         = float(hyperparams[\"decay\"])\n",
    "burn_in       = int(hyperparams[\"burn_in\"])\n",
    "\n",
    "lr=0.0001\n",
    "# optimizer = torch.optim.Adam([\n",
    "#                 {'params': filter(lambda p: p.requires_grad, model.parameters()),'lr': learning_rate},\n",
    "#                  {'params': all_kernel_param, 'lr': learning_rate}\n",
    "#             ])\n",
    "optimizer = torch.optim.SGD([\n",
    "                {'params': filter(lambda p: p.requires_grad, model.parameters())},\n",
    "                 {'params': all_kernel_param}\n",
    "            ],lr=lr, momentum=momentum, weight_decay = decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from train_utils import *\n",
    "losses = AverageMeter()\n",
    "log_hand=RecordLoss([losses],[[]],100)\n",
    "mb = master_bar(range(10))\n",
    "mb.names = ['tloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianning/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    }
   ],
   "source": [
    "for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        targets = Variable(targets.type(Tensor), requires_grad=False)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['b2', 'b3', 'b1', 'k2', 'k3', 'k1'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_k_b_pool['motorbike'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始检查weight,bais的对应关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x80=model.get_layer_sin_output(imgs,80)\n",
    "x81=model.get_layer_sin_output(imgs,81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([255, 1024, 1, 1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name=['x','y','w','h','conf']+classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store={}\n",
    "for name in key_name:\n",
    "    store[name]={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module_list.81.conv_81.weight\n",
      "module_list.81.conv_81.bias\n",
      "module_list.93.conv_93.weight\n",
      "module_list.93.conv_93.bias\n",
      "module_list.105.conv_105.weight\n",
      "module_list.105.conv_105.bias\n"
     ]
    }
   ],
   "source": [
    "for name,p in model.named_parameters():\n",
    "    if 'conv_81' in name:\n",
    "        if 'weight' in name:weight81=p\n",
    "        if 'bias' in name:    bias81=p\n",
    "        print(name)\n",
    "    if 'conv_93' in name:\n",
    "        if 'weight' in name:weight93=p\n",
    "        if 'bias' in name:    bias93=p\n",
    "        print(name)\n",
    "    if 'conv_105' in name:\n",
    "        if 'weight' in name:weight105=p\n",
    "        if 'bias' in name:    bias105=p        \n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_weight=weight81.view(3,-1,1024,1,1).permute(1,0,2,3,4)\n",
    "shift_bias  =bias81.view(3,-1).permute(1,0)\n",
    "for name,k,b in zip(key_name,shift_weight,shift_bias):\n",
    "    store[name]['k1'] =k\n",
    "    store[name]['b1'] =b\n",
    "shift_weight=weight93.view(3,-1,512,1,1).permute(1,0,2,3,4)\n",
    "shift_bias  =bias93.view(3,-1).permute(1,0)\n",
    "for name,k,b in zip(key_name,shift_weight,shift_bias):\n",
    "    store[name]['k2'] =k\n",
    "    store[name]['b2'] =b\n",
    "shift_weight=weight105.view(3,-1,256,1,1).permute(1,0,2,3,4)\n",
    "shift_bias  =bias105.view(3,-1).permute(1,0)\n",
    "for name,k,b in zip(key_name,shift_weight,shift_bias):\n",
    "    store[name]['k3'] =k\n",
    "    store[name]['b3'] =b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(store,'coco_class_kernel.weight.bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= model([imgs,feed_cls_k_b,80])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1200, 13, 13])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.conv2d(x,full_kernel,bias=full_bias).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1024, 1, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_cls_k_b[0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 80, 507, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model([imgs,feed_cls_k_b,82])[0];out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "last=model.state_dict()['module_list.0.batch_norm_0.running_mean'].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now =model.state_dict()['module_list.0.batch_norm_0.running_mean'].cpu().numpy()\n",
    "(now-last).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()\n",
    "for module in model.modules():\n",
    "    if isinstance(module, torch.nn.modules.BatchNorm1d):\n",
    "        module.eval()\n",
    "    if isinstance(module, torch.nn.modules.BatchNorm2d):\n",
    "        module.eval()\n",
    "    if isinstance(module, torch.nn.modules.BatchNorm3d):\n",
    "        module.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianning/.local/lib/python3.6/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/30, Batch 52/117264 Losses:total 20.10, conf 0.75,x 0.89,y 9.83,w 5.45,h 3.19, r 0.0000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-84ef8085f644>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps=0\n",
    "for epoch in mb:\n",
    "    cur_lr = adjust_learning_rate(lr, optimizer, epoch, gamma=0.1)\n",
    "    for batch_i, (_, imgs, targets) in enumerate(dataloader):\n",
    "        imgs = Variable(imgs.type(Tensor))\n",
    "        targets = Variable(targets.type(Tensor), requires_grad=False)\n",
    "        loss = model([imgs,feed_cls_k_b], targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "      \n",
    "        \n",
    "        log_hand.step(steps,[loss])\n",
    "        #log_hand.update_graph(mb,steps)\n",
    "        print(\n",
    "            \"Epoch {}/{}, Batch {}/{} Losses:total {:.2f}, conf {:.2f},x {:.2f},y {:.2f},w {:.2f},h {:.2f}, r {:.1f}\"\n",
    "            .format(\n",
    "                epoch,\n",
    "                opt.epochs,\n",
    "                batch_i,\n",
    "                len(dataloader),\n",
    "                loss.item(),\n",
    "                model.losses[\"x\"],\n",
    "                model.losses[\"y\"],\n",
    "                model.losses[\"w\"],\n",
    "                model.losses[\"h\"],\n",
    "                model.losses[\"conf\"],\n",
    "                model.losses[\"recall\"],\n",
    "            ), end=\"\\r\")\n",
    "        if np.isnan(loss.cpu() .detach().numpy()):sys.exit(0)\n",
    "        model.seen += imgs.size(0)\n",
    "        steps += 1\n",
    "        if steps%10==1:\n",
    "            log_hand.print2file(steps,os.path.join(opt.checkpoint_dir,\"loss.log\"))\n",
    "    if epoch % opt.checkpoint_interval == 0:\n",
    "            model.save_weights(\"%s/epoch_%d.weights\" % (opt.checkpoint_dir, epoch))\n",
    "            torch.save(cls_k_b_pool,\"%s/epoch_%d.plug_in_80_class.weight\"% (opt.checkpoint_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def gray_print(img):\n",
    "    plt.imshow(img,'gray')\n",
    "def rgb_print(img):\n",
    "    img_rgb=img[:,:,[2,1,0]]\n",
    "    plt.imshow(img_rgb,'gray')\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4):\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[..., 0]  = prediction[..., 0] - prediction[..., 2] / 2\n",
    "    box_corner[..., 1]  = prediction[..., 1] - prediction[..., 3] / 2\n",
    "    box_corner[..., 2]  = prediction[..., 0] + prediction[..., 2] / 2\n",
    "    box_corner[..., 3]  = prediction[..., 1] + prediction[..., 3] / 2\n",
    "    prediction[..., :4] = box_corner[..., :4]\n",
    "\n",
    "    batches,classes,boxes,_ = prediction.shape\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred_class in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        for c in range(classes):\n",
    "            image_pred =image_pred_class[c] \n",
    "            # located at each class\n",
    "            conf_mask  = (image_pred[:, 4] >= conf_thres).squeeze()\n",
    "            image_pred = image_pred[conf_mask]\n",
    "            # If none are remaining => process next image\n",
    "            if not image_pred.size(0):continue\n",
    "            # Detections ordered as (x1, y1, x2, y2, obj_conf, class_conf, class_pred)\n",
    "            detections = image_pred\n",
    "            # Iterate through all predicted classes\n",
    "            # Get the detections with the particular class\n",
    "            detections_class = detections\n",
    "            # Sort the detections by maximum objectness confidence\n",
    "            _, conf_sort_index = torch.sort(detections_class[:, 4], descending=True)\n",
    "            detections_class = detections_class[conf_sort_index]\n",
    "            # Perform non-maximum suppression\n",
    "            max_detections = []\n",
    "            while detections_class.size(0):\n",
    "                # Get detection with highest confidence and save as max detection\n",
    "                max_detections.append(detections_class[0].unsqueeze(0))\n",
    "                # Stop if we're at the last detection\n",
    "                if len(detections_class) == 1:break\n",
    "                # Get the IOUs for all boxes with lower confidence\n",
    "                ious = bbox_iou(max_detections[-1], detections_class[1:])\n",
    "                # Remove detections with IoU >= NMS threshold\n",
    "                detections_class = detections_class[1:][ious < nms_thres]\n",
    "\n",
    "            max_detections = torch.cat(max_detections).data\n",
    "            max_detections = F.pad(max_detections,(0,1,0,0),value=c)\n",
    "            # Add max detections to outputs\n",
    "            output[image_i] = (\n",
    "                max_detections if output[image_i] is None else torch.cat((output[image_i], max_detections))\n",
    "            )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def naive_position(prediction):\n",
    "    box_corner = prediction.new(prediction.shape)\n",
    "    box_corner[..., 0]  = prediction[..., 0] - prediction[..., 2] / 2\n",
    "    box_corner[..., 1]  = prediction[..., 1] - prediction[..., 3] / 2\n",
    "    box_corner[..., 2]  = prediction[..., 0] + prediction[..., 2] / 2\n",
    "    box_corner[..., 3]  = prediction[..., 1] + prediction[..., 3] / 2\n",
    "    prediction[..., :4] = box_corner[..., :4]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#model.train()\n",
    "_=model.eval() # Set in evaluation mode\n",
    "\n",
    "cls_k_b_pool=torch.load('checkpoints/latest.plug_in.weights')\n",
    "feed_cls_k_b=review_plug_in(cls_k_b_pool,classes,cuda)\n",
    "\n",
    "trn_path=train_path.replace('trainvalno5k','train5k')\n",
    "val_path=train_path.replace('trainvalno5k','val5k')\n",
    "\n",
    "opt.batch_size=10\n",
    "dataloader = torch.utils.data.DataLoader(ListDataset(train_path), batch_size=opt.batch_size, shuffle=False, num_workers=opt.n_cpu)\n",
    "classes = load_classes(opt.class_path) # Extracts class labels from file\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for batch_i, (img_paths, input_imgs, targets) in enumerate(dataloader):\n",
    "    img_path =img_paths\n",
    "    input_img=input_imgs\n",
    "    target =targets.cpu().numpy().reshape(-1,5)\n",
    "    #print(targets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get detections\n",
    "with torch.no_grad():\n",
    "    input_imgs = Variable(input_img.type(Tensor))\n",
    "    test = model([input_imgs,feed_cls_k_b,81])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Get detections\n",
    "with torch.no_grad():\n",
    "    input_imgs = Variable(input_img.type(Tensor))\n",
    "    detections = model([input_imgs,feed_cls_k_b])\n",
    "#detections = non_max_suppression(detections, 0.95, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'naive_position' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d91a75c84dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnaive_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'naive_position' is not defined"
     ]
    }
   ],
   "source": [
    "detection=naive_position(detections[0]);detection.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,class_pred in enumerate(detection):\n",
    "    if torch.max(class_pred[:, 4]) > 0.9:\n",
    "        print(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.max(detection[16][:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes.index('dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layer.bce_loss(pred_conf, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image_pred = detection\n",
    "conf_mask  = (image_pred[:, 4] >= 0.9 ).squeeze()\n",
    "image_pred = image_pred[conf_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rgb_print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ll=draw_box_with_label(img_flow,[tdet])\n",
    "rgb_print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def back2origin(x1,y1,x2,y2,pad_x,pad_y,gain,scale):\n",
    "    x1-= pad_x\n",
    "    x2-= pad_x\n",
    "    y1-= pad_y\n",
    "    y2-= pad_y\n",
    "    x1/= gain\n",
    "    x2/= gain\n",
    "    y1/= gain\n",
    "    y2/= gain\n",
    "    c_x,c_y,w,h=(x1+x2)/2,(y1+y2)/2,(x2-x1),(y2-y1)\n",
    "    w*=scale\n",
    "    h*=scale\n",
    "    x1,y1,x2,y2=c_x-w/2,c_y-h/2,c_x+w/2,c_y+h/2\n",
    "    return int(x1),int(y1),int(x2),int(y2)\n",
    "def draw_box_with_label(img_origin,detections,input_img_size=416.0):\n",
    "    img=img_origin.copy()\n",
    "    img0_shape=img.shape\n",
    "    img_size=416\n",
    "    scale=1.0*img_size/input_img_size\n",
    "    print(scale)\n",
    "    gain = float(img_size) / max(img0_shape)  # gain  = old / new\n",
    "    pad_x = (img_size - img0_shape[1] * gain) / 2  # width padding\n",
    "    pad_y = (img_size - img0_shape[0] * gain) / 2  # height padding\n",
    "    \n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        detections    = detections[0]\n",
    "        unique_labels = detections[:, -1]\n",
    "        n_cls_preds   = len(unique_labels)\n",
    "        bbox_colors   = (0,255,0)\n",
    "        for x1, y1, x2, y2, cls_pred,cls_conf in detections:\n",
    "            conf=cls_conf\n",
    "            x1,y1,x2,y2=back2origin(x1,y1,x2,y2,pad_x,pad_y,gain,scale)\n",
    "            color =(0,255,0)\n",
    "            _=cv2.rectangle(img, (x1,y1), (x2, y2), color, 2)  \n",
    "            _=cv2.putText(img, classes[int(cls_pred)], (x1,y1), cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
